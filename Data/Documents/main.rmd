---
title: "Cointegration in Crude Prices"
author: "Cameron Pfiffer"
date: "July 17, 2017"
output:
  bookdown::pdf_document2:
    latex_engine: xelatex
csl: harvard.csl
bibliography: bibliography.bibtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, include=FALSE,
                      warning = FALSE, error = FALSE, message = FALSE)
```

```{r include=FALSE}
# Libraries
library(tidyverse)
library(xts)
library(urca)
library(tseries)
library(tidyquant)
```

```{r include=FALSE}
# Functions
create_xts <- function(df) {
  df <- xts(df$price, df$date)
  return(df)
}

create_xts2 <- function(df) {
  # Use for bid/ask datasets
  df <- xts(df[,2:3], order.by=df$date)
  return(df)
}
```

\pagebreak

# Introduction
Determining whether global oil markets are cointegrated is an important question, as doing so would allow a market participant to formulate trading strategies in cases where short-term relationships break down. It makes economic sense for the two oil grades to be cointegrated -- they are essentially substitutes, with similar specific gravities and sulfur content. Indeed, Adelman [-@10.2307/41321691] claimed that "The world oil market, like the world ocean, is one great pool." This study reviews existing literature, examines the statistical relationship between Brent crude and West Texas Intermediate (WTI) crude, and proposes a trading strategy based on cointegration between the two securities.

# Literature Review

## Cointegration
Cointegration as a concept was first introduced by Granger [-@granger1981some], and later expanded upon to include methodology for testing cointegration [@engle1987co]. The theory states that non-stationary series integrated of the same order may have a cointegrating vector that reduces the resulting series to a stationary process, in which case the series are said to be _cointegrated_. Assets linked by a fundamental relationship have cause to be cointegrated, and the crude oil markets are no exception. WTI and Brent crudes are both highly similar commodities, with proximate specific gravities and sulfur contents. It makes sense that two nearly fungible assets would trade at similar levels, ignoring cost of carry and transit costs. Johansen [-@10.2307/2938278] later expanded upon the Engle-Granger methodology to expand to multivariate cointegrating relationships, and offered a solution to the issue of declaring one variable dependent on the other. This paper uses both methodologies to test for cointegration.

There are consistent findings amongst academics that WTI and Brent are cointegrated, and that oil levels are $I(1)$, but stationary in first differences, consistent with expectations. Hammoudeh et al. [-@showkat08] found cointegrating relationships in Brent and WTI (as well as Dubai and Maya crudes) using both the Engle-Granger method [-@engle1987co] and the M-TAR approach [@doi:10.1080/07350015.1998.10524769; @RePEc:bes:jnlbes:v:19:y:2001:i:2:p:166-76] which permits asymmetry in the return to equilibrium levels[^symmetry].  Fattouh [-@FATTOUH2010334] finds similar cointegrating relationships, though notes the presence of structural breaks and non-stationarity in crude price differentials in some extraordinary time periods. Numerous other academics have confirmed the existence of cointegration [@10.2307/41322733; @REBOREDO2011948; @kim2009study; @azar2017bias] using a variety of methodologies.

[^symmetry]: Hammoudeh [-@showkat08] notes that there is cause to suspect asymmetrical returns to equilibrium; he cites the heterogeneity in global trader's expectations, compulsive and noisy trading, and transaction costs as potential factors. Asymmetry is highly important for trading strategies, though for simplicity, the cointegrating relationship between Brent and WTI is assumed to be symmetric.

## Trading Strategy and Transaction Costs
Developing a trading strategy to requires the awareness of several key facts. First, in a world absent transaction costs, any deviation from the mean should spur immediate mean reversion no matter how small, as noted by Balke and Famby [-@10.2307/2527284]. However, as we do not live in a world absent transaction costs and this is an empirical study (and thus subject to the whims of reality), an appropriate transaction cost must be assumed to evaluate the profitability of any trade. In this study, transaction costs are included by construction, as the trading simulation assumes that all purchases are make on the ask and all sales made on the bid. This is an overly simplified assumption, as this merely accounts for explicit transaction costs, and not implicit costs such as opportunity cost and market impact, among others[^tca].

[^tca]: See Almgren and Chriss [-@almgren2001optimal] and Kissel [-@kissell2006expanded] for expanded transaction cost analysis methods.

Kawasaki et al. [-@kawasaki2003characterization] study contrarian and momentum trading strategies in cointegrated equities. Given that integrated series cross the mean frequently, the authors note that there are two general outcomes; first, that the spread between assets might continue away from the mean due to momentum; second, that an asset will revert towards the mean. They find Sharpe ratios of approximately 0.90 for the momentum strategy, and 0.59 for the contrarian strategy, though it should be noted that the contrarian strategy had far higher average returns, 10.69% versus the momentum strategy's 6.46%.


# Statistical Tests

## Data

The data used are daily close bid, ask, and midpoint prices for Dated Brent FOB and West Texas Intermediate FOB between January 1st, 1995 and July 14th, 2017, composing a total of 5,571 observations. Days when prices for either security was not available are removed, so that only days when both securities are traded are used. Furthermore, the data is segmented into two parts; an in-sample and out-of-sample set. The cointegrating relationships, trading strategy, and other estimates are performed on the in-sample data between January 1st, 1995 and July 14th, 2015. The remaining two years of data is used to evaluate the trading strategy using the cointegrating vectors found in the preceding period.

```{r include=FALSE}
# Data import, cleaning, consolidation.
brent <- read_csv("../raw/BFO.csv")
brent$date <- parse_date(brent$date, "%d/%m/%Y")
brent <- create_xts(brent)

wti <- read_csv("../raw/WTI.csv")
wti$date <- parse_date(wti$date, "%d/%m/%Y")
wti <- create_xts(wti)

# Now we import bid/ask spreads and some other stuff.
bidask_brent <- read_csv("../raw/BFObidask-daily.csv")
bidask_brent$date <- parse_date(bidask_brent$date, "%m/%d/%Y")
bidask_brent <- create_xts2(bidask_brent)
bidask_brent$spread <- bidask_brent$ask - bidask_brent$bid

bidask_wti <- read_csv("../raw/WTIbidask-daily.csv")
bidask_wti$date <- parse_date(bidask_wti$date, "%m/%d/%Y")
bidask_wti <- create_xts2(bidask_wti)
bidask_wti$spread <- bidask_wti$ask-bidask_wti$bid

bidask_spread <- merge(bidask_wti, bidask_brent)
colnames(bidask_spread) <- c("w.ask", "w.bid", "w.spread",
                      "b.ask", "b.bid", "b.spread")

price <- na.omit(merge(wti, brent, bidask_spread))

# Format total dataframe in OLS format
p_tib <- as_tibble(price)

p_tib <- mutate(p_tib,
          wti.dl1 = diff.xts(wti),
          brent.dl1 = diff.xts(brent),
          wti.l1  = lag.xts(wti),
          brent.l1 = lag.xts(brent),
          constant = 1)

p_tib <- xts(p_tib, order.by = index(price))["1995/"]


test_set <- price["2015-07-15/"]
# Remove mismatched time series and remove NA
price <- price["1995/2015-07-14"]
```

```{r messages=FALSE}


```


## Lag Length Selection

It has been noted that lag length selection can have important effects on a VAR model's impulse response and variance decompositions [@66e755df06db415c83234a6bbc768a39], and Brooks [-@brooks2014introductory] notes that Johansen tests and Augmented Dickey-Fuller tests can be sucestible to improper lag selection. Thus, we select a lag length that creates produces the most parsimonious model, i.e., the smallest lag length. The `vars` package for R [@pfaff2013package] contains a built in function for multivariate information criterion. Table \@ref(tab:ic) presents the lag lengths suggested by four information criteria tests. The Schwarz criterion suggests a lag length of 2.

```{r}
library(vars)
lag_length <- VARselect(price, type = 'const')
ll_table <- data.frame(lag_length$selection)
```

```{r ic,include=TRUE}
knitr::kable(ll_table, col.names = "Lag Length", caption = "Information Criteria Lag Length", format = 'pandoc')
```

## Stationarity

The first step in determining whether two assets are cointegrated is to find whether they are both $I(0)$. It is important to find unit roots in the levels of each potentially cointegrated series -- if only one of two series is $I(1)$ and the other is $I(0)$, there can be no cointegrting vector and no inferences about the long term relationship between the two assets can be made.

Table \@ref(tab:adfsummary) presents summary statistics for each series using lag $k=2$, with the results clearly showing unit roots in both series. This is the expected result; few would look at a pricing chart and state that it is stationary.

```{r}
wti_adf <- adf.test(price$wti, k=2)
brent_adf <- adf.test(price$brent, k=2)

summarise_adf <- function(adf) {
  adf_sum <- c(adf$statistic, adf$p.value)
  names(adf_sum) <- c("Augmented Dickey-Fulley Statistic", "P Value")
  return(adf_sum)
}

wti_adf_summary <- summarise_adf(wti_adf)
brent_adf_summary <- summarise_adf(brent_adf)
adf_table <- t(data.frame(wti_adf_summary, brent_adf_summary))
row.names(adf_table) <- c("WTI", "Brent")
```

```{r adfsummary, include=TRUE}
knitr::kable(adf_table, caption = "Augmented Dickey-Fuller Summary",
             format = 'pandoc')
```



## Testing for Cointegration

Two common methodologies in testing for cointegration are the Engle-Granger methodology, and the Johansen test. The Engle-Granger procedure is simple, though it suffers from some issues that cannot be solved by large sample sizes. First, hypothesis testing about the cointegrating relationship cannot be performed using the stage 1 regression, though for the purposes of this study, this is not a large issue. The goal here is to describe the relationship, not evaluate a prior expectation or challenge a thesis. The second problem is that there may be multiple cointegrating relationships due to multiple variables, though, again, this is not an issue as we are only testing two variables. Thus, the Engle-Granger method is sufficient for explaining the relaitonship between the two series.

The Johansen test is used primarily as an educational exercise, and to compare results of the cointegrating relationship. As the primary purpose of generating the cointegrating series is to forecast returns to the mean and not simply to explain movements in a series, the Engle-Granger method is perhaps less appropriate. ===================


The results for the Engle-Granger method are as follows. The first step involves estimating a model of the form:

$$\text{wti}_t =\alpha + \beta_1 \text{brent}_t + \epsilon_t$$

Next, a unit root test is run on the $\epsilon$ term to determine that it is stationary. If the null hypothesis of stationarity is rejected, the residuals are stationary, and thus WTI and Brent are cointegrated. The residuals are then used in the error correction model below:

$$\Delta \text{wti}_t=\alpha + \beta_1\Delta \text{brent}_t+\beta_2(\hat{\epsilon}_{t-1})+v_t$$

Where $\hat{\epsilon}_{t-1}$ is the residuals estimated in the first step. The intuition for the model is fairly simple - once the cointegrating relationship ($\beta_1$) is estimated, the coefficient of the lagged error ($\beta_2$) represents the "speed" at which past errors are corrected. It should also be noted that an intercept ($\alpha$) is included in both the preceding equations, as there is little cause to exclude the intercept from the regression, and the intercept term is insignificant anyway. Thus, if $\hat{\epsilon}_{t-1}$ is large and $\beta_2$ is negative (as expected[^beta2]), there will be (on average) a revision towards the mean in the opposite direction of the error.  Table \@ref(tab:egsummary) displays the results of the residuals test, and Table \@ref(tab:egmodel) summarises the results of the error correction model.

[^beta2]: It's highly unlikely that $\beta_2$ would ever be positive, as that represents an explosive process, and the process wouldn't be stationary.


```{r}
eg <- lm(formula = (wti) ~ (brent), data = price)
eg_adf <- adf.test(eg$residuals, k=2)
eg_dat <- data.frame(diff(price$wti), diff(price$brent), lag(eg$residuals))
eg_correction <- lm(wti ~ brent + lag.eg.residuals.,
                    na.action = na.omit,data = eg_dat)



epsilon <- data.frame(eg_adf$statistic, eg_adf$p.value)
colnames(epsilon) <- c("Augmented Dickey-Fulley Statistic", "P Value")
row.names(epsilon) <- "$\\epsilon$"

eg_summary <- summary(eg_correction)
eg_coef <- eg_summary$coefficients
row.names(eg_coef) <- c("Intercept ($\\alpha$)",
                        "Brent ($\\beta_1$)",
                        "Residuals ($\\epsilon$)")
```

```{r egsummary, include=TRUE}
knitr::kable(epsilon, caption = "Engle-Granger Residuals Test",
             format = 'pandoc')
```




```{r egmodel, include=TRUE}
knitr::kable(eg_coef, caption = "Engle-Granger Error Correction Model Summary",
             format = 'pandoc')
```

As Table \@ref(tab:egmodel) demonstrates, the coefficients on the Brent is significant, as would be expected of a cointegrated relationship, and the coefficient on the residual is negative and significant.

It's also important to note that, because the error correction model is estimated using OLS, ensuring that the assumptions of the estimator are not violated is worth a cursory check.


```{r}
# Test OLS stuff
ass1 <- "$\\text{E}(\\epsilon_t)=0$"

ass2 <- "$\\text{var}(u_t)=\\sigma^2 < \\infinity$"
white <- white.test(as.ts(eg$residuals))

ass3 <- "$\\text{cov}(u_t, u_j)=0$"
ass4 <- "$\\text{cov}(u_t,x_t) = 0$"
ass5 <- "$u_t ~ N(0, \\sigma^2)$"

row1 <- c(ass1, paste("Residual mean is ", mean(eg$residuals)))
row2 <- c(ass1, paste("White test pvalue is ", white$p.value))
```

```{r include=TRUE}
fitted = xts(eg_correction$fitted.values,
             order.by = as.Date(names(eg_correction$fitted.values)))

colnames(fitted) <- "Fitted"


```



```{r}
joc <- ca.jo(price[,1:2], type = 'eigen', ecdet = 'const',
                  K = 2, spec = 'transitory')
summary(joc)
```


\pagebreak


# Trading Strategies

The overview of the trading strategy is that it follows several simple steps.

1. Calculate a stationary series using a given cointegrating vector.
2. Take an appropriate long/short position when the spread is outside a threshold.
3. Close out position when the spread reaches the opposite threshold.
4. Repeat until end of series, and then close out positions.

Using the Johansen test's cointegrating vector, a spread variable is created by calculating the below equation:

$$\text{spread}_t =\alpha+ \text{wti}_t-\beta _1\text{brent}_t$$

Using the cointegrating vector found previously:

$$\text{spread}_t =-5.425+ \text{wti}_t-0.873\text{brent}_t$$

Where $\text{spread}_t$ is the stationary series created by an intercept ($\alpha$), the price of WTI at time $t$, and the price of Brent at time $t$ multiplied by a cointegrating coefficient ($\beta$). Figure \@ref(fig:stationary-series) plots this series for the entire data set. A threshold is then utilized as in Dunis et al. [-@doi:10.1080/09603100500426432], whereby WTI is purchased and Brent is sold if the spread is below the negative threshold (long the spread) or the opposite if the spread is above the positive threshold. 

```{r}
joc@V
```

```{r stationary-series, include=TRUE, fig.cap="Cointegrated Series of WTI and Brent"}
make_cointegrated <- function() {
  y = p_tib$wti.l1
  x = p_tib$brent.l1

  if( dim(joc@V)[1] > 2) {
    const = joc@V[3,1]
  } else {
    const = 0
  }

  y_val = joc@V[1,1]
  x_val = joc@V[2,1]

  coint = y*y_val + x*x_val + const
  return(coint)
}

spread_jo <- make_cointegrated()
ggplot() +
  geom_line(mapping=aes(index(spread_jo), 1), color = 'lightblue', size=1) +
  geom_line(mapping=aes(index(spread_jo), -1), color='lightblue', size=1) +
  geom_line(mapping=aes(index(spread_jo), spread_jo)) +
  xlab("Date") +
  ylab("Spread") +
  theme_minimal()
```

Using this series, 

```{r cache=TRUE, eval=FALSE}
error_jo <- spread_jo - mean(spread_jo)
trade_jo <- sd(spread_jo) - mean(spread_jo)

source('C:/Users/cpfif/Documents/R/Energy Finance/Data/tradefunction2.R')

trades <- trade_determine2(price, 1, 100000, 1000) 
```


# Out of sample test

```{r eval=FALSE}
oos <- trade_determine2(test_set, 1, 100000, 10) 
```







# References
